---
layout: post
title: 推荐引擎和广告检索引擎的异同
category: architecture
---

## 背景

之前做海外短视频的推荐引擎，后面转做竞价广告的检索引擎，所以一直想写下这两者的异同。

<br>

## 关于物料，来源&进入实时流

当时从推荐团队到广告团队的一个原因，就是两者业务逻辑是相似。

都是将物料从召回、过滤、打分、排序、选择(渲染)，这样的流程下发到下游，下游再进行曝光、点击、转化等回流数据到后端。

### 物料量级
从物料量上来讲，我经历的两个团队的物料量是不在一个量级的，推荐的物料量要远远大于广告创意。视频的物料池包括UGC，PGC视频等应该有几十亿之多。广告的物料库不到千万级，如果是某一个时刻能够进行投放的物料，大概只有数万量级。

当然不同的广告团队，业务场景会有很大的不同，不过这相对符合直观的认知，一个平台的内容应该远远高于在这个平台进行投放的广告的。

### 物料来源
从物料的来源的说，推荐的内容来自于用户上传、达人(签约用户)生产，或者从爬虫得到的网络资源。内容质量参差不齐，而且五花八门，所以会经过机神、多层人工审核、复审、打标等等流程。

但是广告创意就会对内容的把控明确得多，广告主上传的创意绝大部分是符合进行投放广告位的要求的，比如图片/视频的宽高/码率等，如果出现了这样的情况，基本在广告创意阶段就会在创建阶段就会拦截，返回给客户不合规的具体情况；以及广告内容也是基本满足要求(很少出现涉黄涉政等不合规的情况)，同时在进行投放的时候，客户也能够主动选择定向、行业等信息，一定程度来补充物料信息，在通过人工审核等基本就能够投放要求。

### 物料入库下发
推荐的物料来源从五花八门，所以会经过层层审核。因为推荐场景更多的是关注的物料本身，而且物料的分发和广场场景有很大的不同，接近于高分者得(当然也会有用户兴趣ee等场景，所以会让低分物料下发)。

物料入库随即就会进行审核打标，使物料获得基本信息；然后进行物料的探索，一般来说会进行多轮的探索（但是在某些不太精细的场景也只有一轮）。

如果给足了一定的探索机会(一般来说是曝光额度)，物料没有达到预期的指标，如ctr,cvr,互动率等等，物料就会进入物料库，再下一轮索引构建的时候通过其他方式才有可能下发；而达到预期指标的物料，会进入推荐的常规池或者热播池，属于是相对精品的内容，可以分发到用户侧，提升用户的指标vv，ts等。另外提一下，探索视频往往是降指标的，所以一般也不会给新用户或者易流失用户下发探索视频。

其实热播视频，也会经历几个热播池，最后来到高热池。此时往往会进入到运营干预阶段，给物料降权，加权，或者进行内容运营（热搜，话题，造势）等等，将运营获取得到的信息再进入feed流分发，同时内容运营也会在其他的流量入口分发流量。

![推荐物料来源1](http://blogcdn.qihope.com/github-blog-pic/2022-03-24-what-is-diff-between-recommend-and-ad-1.png)
![推荐物料来源2](http://blogcdn.qihope.com/github-blog-pic/2022-03-24-what-is-diff-between-recommend-and-ad-2.png)

![广告物料来源](http://blogcdn.qihope.com/github-blog-pic/2022-03-24-what-is-diff-between-recommend-and-ad-3.png)

<br>

### 离线计算：
- 根据打点日志，计算用户行为，得出用户的离线画像
- 根据打点日志，计算视频的离线特征，包括用户的点击展现数据(CTR)，视频的互动数据(ATR)
- 从媒资库获取视频基础信息，获取到近期或全量视频，生成各个视频索引文件(索引拉链)

<br>

### 实时计算：
- 根据MQ实时消息，计算视频的实时特征，包括实时ctr，atr，各项互动数据等
- 根据MQ实时消息，计算用户的实时特征，获取用户的偏好信息，对某一品类/作者的喜好等反馈
- 根据实时用户信息，从索引文件中获取视频列表，实时计算视频分数，推荐给用户

<br>

![推荐feed流实时离线架构](http://blogcdn.qihope.com/github-blog-pic/2019-02-09-recommend-engine.png)

离线部分主要交给了gump任务平台去完成这类的计算任务，gump下主要为hadoop。

实时部分分为了两个服务，一个是engine，负责获取各类特征，并完成计算将最终结果返还给调用方；一个是feature服务，负责计算视频特征，用户特征，提供给engine使用。

注：离线平台完成计算之后，会生成一张hive表，两个实时服务engine和feature是将生成完成的hive表拷贝到服务机器上，进行解析然后存入对应的缓存、内存。

## engine服务主要架构

engine的主要任务是获取到不同的视频索引(包含视频的离线特征)，视频的实时特征，用户特征等，安装既定的公式/算法得出视频的排序列表，再根据服务的机制，筛选出定长的视频列表返回给服务调用方。

![engine服务flow](http://blogcdn.qihope.com/github-blog-pic/2019-02-09-recommend-engine-flow.png)

主要这几个部分：

上下文信息填充/ab-test准备，召回，过滤，特征合并，排序，选择/机制，填充

<br>

### 1. 上下文信息填充/ab-test准备

推荐上游调用推荐API，此时会传入用户请求的基本信息，如版本号，设备id，网络情况，请求方式等等，推荐服务将各种信息填入上下文，并且根据已配置好的ab测试配置，讲请求分类填入请求上下文。

### 2. 召回

召回是根据获取各个视频拉链，为后续的过滤排序阶段提供基础的视频池。

召回的视频拉链主要分为三类：概括为获取用户画像，根据用户画像召回视频拉链；根据视频离线特征召回基础底池视频拉链；召回特殊拉链。

目前线上运行的engine服务存在的主要索引有：High-quality索引，Uploader索引(根据视频创作者建立的索引)，Tag索引(根据视频品类建立的索引)，I2I索引(根据视频的内容偏好相似度建立的索引，协同过滤计算得出)，Explore索引，Start索引(主要为新用户建立的索引拉链)，New索引(保障新进入媒资库的视频，能够具有一定的展现机会，来判断这个视频是否为优质视频)

### 3. 过滤

很多视频尽管已经进入了视频的离线索引，但是也会根据时间不同、规则不同、用户不同，不能进入推荐列表，所以需要进行过滤，在过滤之前还加上了截断来保障视频过滤更加高效，但可能导致尾部视频一直无法出现到推荐feed中。

目前线上运行的主要过滤规则有：黑名单视频过滤(相较离线更加迅速，过滤更加直接)、黑名单Tag过滤，特殊分支过滤(不同策略导致某些已上线分支不能进推荐列表)、音乐类别过滤、图集过滤、用户已播放视频过滤、用户已展现视频过滤、上传者过滤(用户在feed流不能浏览到自己上传的视频)、新视频保量过滤(某些已得到了充分展现的视频将会被过滤掉并且从new分支移除)、相似视频过滤

### 4. 实时特征获取

根据已经过滤完成的索引列表中视频，从feature服务(接口或者存储耦合)获取视频实时特征，并填充进索引列表，上下文特征。因为在召回阶段已经获取了一部分用户特征了，所以此处只用获取视频侧的特征。

### 5. 视频排序

根据获取得到的视频特征，按照既定的公式排序，主要参考的特征有视频实时ctr，视频播放完成率，视频实时互动分，用户特征分数。

视频的排序目前是按各个分支独立排序。

### 6. 视频选择

由于是视频的各个分支独立排序，所以出视频的视频，需要按照给定的槽位分配到各个分支，然后按照各个分支的排序完成后的视频列表从高到低依次选择槽位数的视频。

其中需要注意的是：分支之间需要判重，品类数量控制，特殊视频的数量控制(特殊视频可能出现在各个分支)。

同时也需要进行槽位控制，不同的用户的分支槽位可能是存在差别大，比如新用户、冷起用户 画像索引会非常少甚至没有，那么相应的冷起索引就需要更多的槽位。

### 7. 填充

在视频选择完成之后，就基本上获取得到了推荐所需的视频列表。但是仍然存在可能是某个分支的视频数量少于了给定的槽位的视频数量。此时就需要将底池视频补齐。并且在完成推荐之后，还有诸多的视频信息、推荐信息、推荐策略、推荐获取到的实时特征等填充进推荐列表，并且完成发送MQ消息，打印Debug信息等操作。

<br>

## feature服务主要架构

feature服务主要分为两个部分：feature-online, feature-offline

离线部分主要完成将离线任务平台gump得到的计算结果，根据一定的规则导入到feature特征库(redis)。

实时部分主要获取实时MQ消息，计算用户特征/用户画像，视频特征等，提供给engine相应接口。
